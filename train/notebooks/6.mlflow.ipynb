{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import RobertaTokenizer\n",
    "from transformers import RobertaForSequenceClassification\n",
    "from transformers import RobertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = \"./tokenizer_roberta\"\n",
    "model_path = \"./model_roberta\"\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(tokenizer_path)\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "\n",
    "    probabilities = torch.softmax(logits, dim=1).squeeze()\n",
    "\n",
    "    predicted_class = torch.argmax(probabilities).item()\n",
    "    predicted_sentiment = \"positive\" if predicted_class == 1 else \"negative\"\n",
    "\n",
    "    return predicted_sentiment, probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment prédit: positive\n",
      "Probabilités: tensor([0.0429, 0.9571])\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, how are you?\"\n",
    "sentiment, probabilities = predict_sentiment(text)\n",
    "print(\"Sentiment prédit:\", sentiment)\n",
    "print(\"Probabilités:\", probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ./model_roberta and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_path = \"./tokenizer_roberta\"\n",
    "model_path = \"./model_roberta\"\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(tokenizer_path)\n",
    "model = RobertaModel.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_sentence(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    # Calculer les embeddings\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    logits = output.logits\n",
    "    embeddings = model.roberta.embeddings.word_embeddings(input_ids)\n",
    "\n",
    "    # Récupérer les couches d'attention\n",
    "    attentions = output.attentions\n",
    "\n",
    "    return embeddings, attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BaseModelOutputWithPoolingAndCrossAttentions' object has no attribute 'logits'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, how are you?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m embeddings, attentions \u001b[38;5;241m=\u001b[39m \u001b[43minterpret_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[68], line 10\u001b[0m, in \u001b[0;36minterpret_sentence\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      8\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(input_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask)\n\u001b[0;32m---> 10\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m\n\u001b[1;32m     11\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mroberta\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mword_embeddings(input_ids)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Récupérer les couches d'attention\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BaseModelOutputWithPoolingAndCrossAttentions' object has no attribute 'logits'"
     ]
    }
   ],
   "source": [
    "sentence = \"Hello, how are you?\"\n",
    "embeddings, attentions = interpret_sentence(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ./model_roberta and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, how are you?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     42\u001b[0m embeddings, attentions \u001b[38;5;241m=\u001b[39m interpret_sentence(sentence)\n\u001b[0;32m---> 43\u001b[0m \u001b[43mvisualize_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattentions\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[71], line 32\u001b[0m, in \u001b[0;36mvisualize_attention\u001b[0;34m(sentence, attentions)\u001b[0m\n\u001b[1;32m     29\u001b[0m n_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokenizer(sentence)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Créer une heatmap pour chaque couche d'attention\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer, layer_attention \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mattentions\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     33\u001b[0m     plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m     34\u001b[0m     sns\u001b[38;5;241m.\u001b[39mheatmap(layer_attention[\u001b[38;5;241m0\u001b[39m][:n_tokens, :n_tokens], cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mviridis\u001b[39m\u001b[38;5;124m'\u001b[39m, annot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "tokenizer_path = \"./tokenizer_roberta\"\n",
    "model_path = \"./model_roberta\"\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(tokenizer_path)\n",
    "model = RobertaModel.from_pretrained(model_path)\n",
    "model.eval()\n",
    "\n",
    "def interpret_sentence(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    # Calculer les embeddings\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    embeddings = output.last_hidden_state\n",
    "    attentions = output.attentions\n",
    "\n",
    "    return embeddings, attentions\n",
    "\n",
    "def visualize_attention(sentence, attentions):\n",
    "    # Récupérer la longueur de la phrase\n",
    "    n_tokens = len(tokenizer(sentence)['input_ids'])\n",
    "\n",
    "    # Créer une heatmap pour chaque couche d'attention\n",
    "    for layer, layer_attention in enumerate(attentions):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.heatmap(layer_attention[0][:n_tokens, :n_tokens], cmap='viridis', annot=False)\n",
    "        plt.title(f'Layer {layer+1} Attention')\n",
    "        plt.xlabel('To')\n",
    "        plt.ylabel('From')\n",
    "        plt.show()\n",
    "\n",
    "# Utiliser la fonction pour visualiser l'attention\n",
    "sentence = \"Hello, how are you?\"\n",
    "embeddings, attentions = interpret_sentence(sentence)\n",
    "visualize_attention(sentence, attentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>999755</th>\n",
       "      <td>4</td>\n",
       "      <td>1879920060</td>\n",
       "      <td>Thu May 21 23:32:07 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>buddhapest</td>\n",
       "      <td>@NovaWildstar my dear husband will have to go ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793985</th>\n",
       "      <td>0</td>\n",
       "      <td>2326819086</td>\n",
       "      <td>Thu Jun 25 07:33:10 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Sorabu</td>\n",
       "      <td>I wanna go to the beach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418798</th>\n",
       "      <td>4</td>\n",
       "      <td>2057852183</td>\n",
       "      <td>Sat Jun 06 13:47:07 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>cwoffeegirl</td>\n",
       "      <td>@cleff re:hangover - ahhhh I so want to see th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070802</th>\n",
       "      <td>4</td>\n",
       "      <td>1966217564</td>\n",
       "      <td>Fri May 29 17:20:55 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>aminorjourney</td>\n",
       "      <td>Wow. I was featured on AutoBlog Green  Sweet.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918493</th>\n",
       "      <td>4</td>\n",
       "      <td>1753664697</td>\n",
       "      <td>Sun May 10 02:47:30 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mob61uk</td>\n",
       "      <td>@SteveLangton Yes, they clearly relished actin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1492623</th>\n",
       "      <td>4</td>\n",
       "      <td>2069325416</td>\n",
       "      <td>Sun Jun 07 15:40:16 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Arantza92</td>\n",
       "      <td>@LisaHopeCyrus well if u're tired go to sleep ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335722</th>\n",
       "      <td>4</td>\n",
       "      <td>2017084567</td>\n",
       "      <td>Wed Jun 03 07:50:09 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>lauraeatworld</td>\n",
       "      <td>trying to write</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126342</th>\n",
       "      <td>0</td>\n",
       "      <td>1834499716</td>\n",
       "      <td>Mon May 18 03:10:14 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tewitje</td>\n",
       "      <td>Having hayfever when it is raining is so wrong.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1315983</th>\n",
       "      <td>4</td>\n",
       "      <td>2014100940</td>\n",
       "      <td>Wed Jun 03 00:27:46 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>KeiraShakesby</td>\n",
       "      <td>is watching bones b4 work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335103</th>\n",
       "      <td>0</td>\n",
       "      <td>2013712568</td>\n",
       "      <td>Tue Jun 02 23:22:25 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>thejenncarroll</td>\n",
       "      <td>@tcostic My last name has two l's...I can see ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         target          id                          date      flag  \\\n",
       "999755        4  1879920060  Thu May 21 23:32:07 PDT 2009  NO_QUERY   \n",
       "793985        0  2326819086  Thu Jun 25 07:33:10 PDT 2009  NO_QUERY   \n",
       "1418798       4  2057852183  Sat Jun 06 13:47:07 PDT 2009  NO_QUERY   \n",
       "1070802       4  1966217564  Fri May 29 17:20:55 PDT 2009  NO_QUERY   \n",
       "918493        4  1753664697  Sun May 10 02:47:30 PDT 2009  NO_QUERY   \n",
       "...         ...         ...                           ...       ...   \n",
       "1492623       4  2069325416  Sun Jun 07 15:40:16 PDT 2009  NO_QUERY   \n",
       "1335722       4  2017084567  Wed Jun 03 07:50:09 PDT 2009  NO_QUERY   \n",
       "126342        0  1834499716  Mon May 18 03:10:14 PDT 2009  NO_QUERY   \n",
       "1315983       4  2014100940  Wed Jun 03 00:27:46 PDT 2009  NO_QUERY   \n",
       "335103        0  2013712568  Tue Jun 02 23:22:25 PDT 2009  NO_QUERY   \n",
       "\n",
       "                   user                                               text  \n",
       "999755       buddhapest  @NovaWildstar my dear husband will have to go ...  \n",
       "793985           Sorabu                           I wanna go to the beach   \n",
       "1418798     cwoffeegirl  @cleff re:hangover - ahhhh I so want to see th...  \n",
       "1070802   aminorjourney      Wow. I was featured on AutoBlog Green  Sweet.  \n",
       "918493          mob61uk  @SteveLangton Yes, they clearly relished actin...  \n",
       "...                 ...                                                ...  \n",
       "1492623       Arantza92  @LisaHopeCyrus well if u're tired go to sleep ...  \n",
       "1335722   lauraeatworld                                   trying to write   \n",
       "126342          tewitje   Having hayfever when it is raining is so wrong.   \n",
       "1315983   KeiraShakesby                         is watching bones b4 work   \n",
       "335103   thejenncarroll  @tcostic My last name has two l's...I can see ...  \n",
       "\n",
       "[1000 rows x 6 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('../Data/tweets.csv', encoding='latin-1', header=None)\n",
    "data = data.rename(columns={data.columns[0]: 'target'})\n",
    "data = data.rename(columns={data.columns[1]: 'id'})\n",
    "data = data.rename(columns={data.columns[2]: 'date'})\n",
    "data = data.rename(columns={data.columns[3]: 'flag'})\n",
    "data = data.rename(columns={data.columns[4]: 'user'})\n",
    "data = data.rename(columns={data.columns[5]: 'text'})\n",
    "data = data.sample(1000)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "text_list = data['text'].tolist()\n",
    "y_pred = classifier(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tab = []\n",
    "for i in y_pred:\n",
    "    if i['label'] == 'LABEL_1':\n",
    "        y_pred_tab.append(1)\n",
    "    else :\n",
    "        y_pred_tab.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_tab = []\n",
    "for i in range(len(data)):\n",
    "    if data['target'].iloc[i] == 0:\n",
    "        y_test_tab.append(0)\n",
    "    else :\n",
    "        y_test_tab.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "base_accuracy = accuracy_score(y_test_tab, y_pred_tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.838"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_sentence(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    # Calculer les embeddings\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    logits = output.logits\n",
    "    embeddings = model.roberta.embeddings.word_embeddings(input_ids)\n",
    "\n",
    "    attentions = output.attentions\n",
    "\n",
    "    return embeddings, attentions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Go fuck yourself i hate you\"\n",
    "embeddings, attentions = interpret_sentence(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import LayerIntegratedGradients, TokenReferenceBase, visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'captum.attr._utils.visualization' has no attribute 'visualize_text_attention'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvisualization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisualize_text_attention\u001b[49m(sentence, attentions)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'captum.attr._utils.visualization' has no attribute 'visualize_text_attention'"
     ]
    }
   ],
   "source": [
    "visualization.visualize_text_attention(sentence, attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m         plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Utiliser la fonction pour visualiser l'attention\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mvisualize_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattentions\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[58], line 9\u001b[0m, in \u001b[0;36mvisualize_attention\u001b[0;34m(sentence, attentions)\u001b[0m\n\u001b[1;32m      6\u001b[0m n_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokenizer(sentence)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Créer une heatmap pour chaque couche d'attention\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer, layer_attention \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mattentions\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     10\u001b[0m     plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m     11\u001b[0m     sns\u001b[38;5;241m.\u001b[39mheatmap(layer_attention[\u001b[38;5;241m0\u001b[39m][:n_tokens, :n_tokens], cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mviridis\u001b[39m\u001b[38;5;124m'\u001b[39m, annot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_attention(sentence, attentions):\n",
    "    # Récupérer la longueur de la phrase\n",
    "    n_tokens = len(tokenizer(sentence)['input_ids'])\n",
    "\n",
    "    # Créer une heatmap pour chaque couche d'attention\n",
    "    for layer, layer_attention in enumerate(attentions):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.heatmap(layer_attention[0][:n_tokens, :n_tokens], cmap='viridis', annot=False)\n",
    "        plt.title(f'Layer {layer+1} Attention')\n",
    "        plt.xlabel('To')\n",
    "        plt.ylabel('From')\n",
    "        plt.show()\n",
    "\n",
    "# Utiliser la fonction pour visualiser l'attention\n",
    "visualize_attention(sentence, attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BERT-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
