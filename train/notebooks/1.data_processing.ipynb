{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory analysis of the tweet sentiment expression dataset.\n",
    "\n",
    "### Introduction :  \n",
    "\n",
    "\n",
    "The main objective of our project is to develop an intelligent system capable of analyzing and understanding the sentiment of a message posted by a user on a social network, in this case here [Twitter](https://twitter. com/home?lang=fr). Using advanced natural language processing (NLP) techniques, our solution will seek to extract key information, establish semantic connections and create a representation of user sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import\n",
    "\n",
    "We will use a usual Data Science stack: `numpy`, `pandas`, `sklearn`, `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# graphic representation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# file system management\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data that we will use during our study has the following relative path [\"../Data\"](). We can find in the [./Data]() folder a csv file which includes a list of user tweets. It is in this folder that we will save our dataframes to be able to use them in other operations of our study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', 'tweets.csv']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\"../Data\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the file which will be useful for our study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         target          id                          date      flag  \\\n",
       "0             0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1             0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2             0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3             0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4             0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "...         ...         ...                           ...       ...   \n",
       "1599995       4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996       4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997       4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998       4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599999       4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    user                                               text  \n",
       "0        _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1          scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2               mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3                ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4                 Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "...                  ...                                                ...  \n",
       "1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...  \n",
       "1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...  \n",
       "1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...  \n",
       "1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...  \n",
       "1599999   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
       "\n",
       "[1600000 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../Data/tweets.csv', encoding='latin-1', header=None)\n",
    "data = data.rename(columns={data.columns[0]: 'target'})\n",
    "data = data.rename(columns={data.columns[1]: 'id'})\n",
    "data = data.rename(columns={data.columns[2]: 'date'})\n",
    "data = data.rename(columns={data.columns[3]: 'flag'})\n",
    "data = data.rename(columns={data.columns[4]: 'user'})\n",
    "data = data.rename(columns={data.columns[5]: 'text'})\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file contains 1,600,000 user tweets extracted using Twitter's API. The tweets have been annotated (0 = negative, 4 = positive) and can be used to detect the sentiment of a tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame is characterized by 6 variables:\n",
    "\n",
    "1. target: the sentiment of the tweet (0 = negative, 4 = positive)\n",
    "2. id: tweet id (2087)\n",
    "3. date: tweet date (Sat May 16 23:58:44 UTC 2009)\n",
    "4. flag: The query (LyX). If there is no query then the value will be NO_QUERY\n",
    "5. user: the user who tweeted (robotickilldozr)\n",
    "6. text: the text of the tweet (with LyX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target                                                                                                                      0\n",
      "id                                                                                                                 1467810369\n",
      "date                                                                                             Mon Apr 06 22:19:45 PDT 2009\n",
      "flag                                                                                                                 NO_QUERY\n",
      "user                                                                                                          _TheSpecialOne_\n",
      "text      @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "    print(data.iloc[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply text simplification by removing user identification, web links, single characters, numeric characters, non-alphanumeric characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "data['text'] = data['text'].apply(lambda x: re.sub(r'\\S*@\\S*\\s?', '', x))\n",
    "data['text'] = data['text'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "data['text'] = data['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "data['text'] = data['text'].apply(lambda x: re.sub(r'\\b\\w\\b', '', x))\n",
    "data['text'] = data['text'].apply(lambda x: re.sub(r'\\d', '', x))\n",
    "data['text'] = data['text'].apply(lambda x: re.sub(r'\\s+', ' ', x))\n",
    "data['text'] = data['text'].apply(lambda x: x.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target                                                                       0\n",
      "id                                                                  1467810369\n",
      "date                                              Mon Apr 06 22:19:45 PDT 2009\n",
      "flag                                                                  NO_QUERY\n",
      "user                                                           _TheSpecialOne_\n",
      "text       awww thats bummer you shoulda got david carr of third day to do it \n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "    print(data.iloc[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nltk library allows us to remove stopwords. These are words that recur in a language but do not provide additional information for understanding the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/gaeldelescluse/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kl/3nc79ycx61v13jbqlw61nnfw0000gn/T/ipykernel_17808/2802848969.py:4: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  data['text_token'][i] = nltk.word_tokenize(data['text'][i])\n",
      "/var/folders/kl/3nc79ycx61v13jbqlw61nnfw0000gn/T/ipykernel_17808/2802848969.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['text_token'][i] = nltk.word_tokenize(data['text'][i])\n",
      "/var/folders/kl/3nc79ycx61v13jbqlw61nnfw0000gn/T/ipykernel_17808/2802848969.py:4: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['awww', 'thats', 'bummer', 'you', 'shoulda', 'got', 'david', 'carr', 'of', 'third', 'day', 'to', 'do', 'it']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  data['text_token'][i] = nltk.word_tokenize(data['text'][i])\n"
     ]
    }
   ],
   "source": [
    "data['text_token'] = 0\n",
    "\n",
    "for i in range(len(data['text'])):\n",
    "    data['text_token'][i] = nltk.word_tokenize(data['text'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(word_list):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = []\n",
    "    for word in word_list:\n",
    "        if word not in stop_words:\n",
    "            filtered_words.append(word)\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data['text_token'])):\n",
    "    data['text_token'][i] = remove_stopwords(data['text_token'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target                                                                           0\n",
      "id                                                                      1467810369\n",
      "date                                                  Mon Apr 06 22:19:45 PDT 2009\n",
      "flag                                                                      NO_QUERY\n",
      "user                                                               _TheSpecialOne_\n",
      "text           awww thats bummer you shoulda got david carr of third day to do it \n",
      "text_token            [awww, thats, bummer, shoulda, got, david, carr, third, day]\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "    print(data.iloc[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/gaeldelescluse/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/gaeldelescluse/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/gaeldelescluse/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('punkt')# segmentation phrases\n",
    "nltk.download('averaged_perceptron_tagger') # étiquettes grammaticales\n",
    "nltk.download('wordnet')# synonymes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization allows us to simplify the meaning of sentences by finding the root of each word. We thus remove the conjugation or even the plural from each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_words(word_list):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, get_pos(word)) for word in word_list]\n",
    "    return lemmatized_words\n",
    "\n",
    "def get_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV, \"J\": wordnet.ADJ}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data['text_token'])):\n",
    "    data['text_token'][i] = lemmatize_words(data['text_token'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dataset = '../Data/normalized_dataset.csv'\n",
    "data.to_csv(word_dataset, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Data/normalized_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "\n",
    "array_list = df['text_token'].values\n",
    "data_list = []\n",
    "for item in array_list:\n",
    "    data_list.append(ast.literal_eval(item))\n",
    "\n",
    "df_list = pd.DataFrame({'text_token': data_list})\n",
    "df = df.drop(columns=['text_token'])\n",
    "df['text_token'] = df_list['text_token']\n",
    "df['words'] = df['text_token'].apply(lambda x: ' '.join(x))\n",
    "df = df.drop(columns=['text_token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target                                                                       0\n",
      "id                                                                  1467810369\n",
      "date                                              Mon Apr 06 22:19:45 PDT 2009\n",
      "flag                                                                  NO_QUERY\n",
      "user                                                           _TheSpecialOne_\n",
      "text       awww thats bummer you shoulda got david carr of third day to do it \n",
      "words                       awww thats bummer shoulda get david carr third day\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "    print(df.iloc[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dataset = '../Data/cleaned_dataset.csv'\n",
    "df.to_csv(word_dataset, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Data/cleaned_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1967456238</td>\n",
       "      <td>Fri May 29 19:36:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ozpancakes</td>\n",
       "      <td>oh well than thats understandable specially si...</td>\n",
       "      <td>oh well thats understandable specially since f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1974019720</td>\n",
       "      <td>Sat May 30 12:16:52 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>IslandBookworm</td>\n",
       "      <td>have friend who thinks it should be civic dut...</td>\n",
       "      <td>friend think civic duty leave one wifi open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1991907659</td>\n",
       "      <td>Mon Jun 01 07:52:24 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>discostickOx</td>\n",
       "      <td>can anybody tell me how to upload picture on t...</td>\n",
       "      <td>anybody tell upload picture thiss say mine big</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1751642499</td>\n",
       "      <td>Sat May 09 19:55:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>sharonhayes</td>\n",
       "      <td>beautiful song for anyone that could use pick...</td>\n",
       "      <td>beautiful song anyone could use pick tonight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2066590713</td>\n",
       "      <td>Sun Jun 07 10:48:27 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Orli</td>\n",
       "      <td>exactly ive told them it reminds me the house ...</td>\n",
       "      <td>exactly ive told reminds house hansel gretel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>0</td>\n",
       "      <td>1678810323</td>\n",
       "      <td>Sat May 02 07:46:58 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>dorothysiok</td>\n",
       "      <td>followed the saga when was supposed to be mugg...</td>\n",
       "      <td>follow saga suppose mug hard guilty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>0</td>\n",
       "      <td>1980260262</td>\n",
       "      <td>Sun May 31 06:12:43 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ichbinkatie</td>\n",
       "      <td>woke up angry</td>\n",
       "      <td>woke angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>1</td>\n",
       "      <td>1833791018</td>\n",
       "      <td>Mon May 18 00:28:24 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>popitlockit</td>\n",
       "      <td>ranch all the way or italian either or</td>\n",
       "      <td>ranch way italian either</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>1</td>\n",
       "      <td>1793829089</td>\n",
       "      <td>Thu May 14 04:07:38 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>canaaa</td>\n",
       "      <td>editing my fs and multiply</td>\n",
       "      <td>edit f multiply</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>1</td>\n",
       "      <td>1970083904</td>\n",
       "      <td>Sat May 30 02:23:14 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>daisymoonellis</td>\n",
       "      <td>had lovely night with emily now what to do today</td>\n",
       "      <td>lovely night emily today</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        target          id                          date      flag  \\\n",
       "0            0  1967456238  Fri May 29 19:36:49 PDT 2009  NO_QUERY   \n",
       "1            1  1974019720  Sat May 30 12:16:52 PDT 2009  NO_QUERY   \n",
       "2            0  1991907659  Mon Jun 01 07:52:24 PDT 2009  NO_QUERY   \n",
       "3            1  1751642499  Sat May 09 19:55:53 PDT 2009  NO_QUERY   \n",
       "4            1  2066590713  Sun Jun 07 10:48:27 PDT 2009  NO_QUERY   \n",
       "...        ...         ...                           ...       ...   \n",
       "999995       0  1678810323  Sat May 02 07:46:58 PDT 2009  NO_QUERY   \n",
       "999996       0  1980260262  Sun May 31 06:12:43 PDT 2009  NO_QUERY   \n",
       "999997       1  1833791018  Mon May 18 00:28:24 PDT 2009  NO_QUERY   \n",
       "999998       1  1793829089  Thu May 14 04:07:38 PDT 2009  NO_QUERY   \n",
       "999999       1  1970083904  Sat May 30 02:23:14 PDT 2009  NO_QUERY   \n",
       "\n",
       "                  user                                               text  \\\n",
       "0           ozpancakes  oh well than thats understandable specially si...   \n",
       "1       IslandBookworm   have friend who thinks it should be civic dut...   \n",
       "2         discostickOx  can anybody tell me how to upload picture on t...   \n",
       "3          sharonhayes   beautiful song for anyone that could use pick...   \n",
       "4                 Orli  exactly ive told them it reminds me the house ...   \n",
       "...                ...                                                ...   \n",
       "999995     dorothysiok  followed the saga when was supposed to be mugg...   \n",
       "999996     ichbinkatie                                     woke up angry    \n",
       "999997     popitlockit            ranch all the way or italian either or    \n",
       "999998          canaaa                        editing my fs and multiply    \n",
       "999999  daisymoonellis  had lovely night with emily now what to do today    \n",
       "\n",
       "                                                    words  \n",
       "0       oh well thats understandable specially since f...  \n",
       "1             friend think civic duty leave one wifi open  \n",
       "2          anybody tell upload picture thiss say mine big  \n",
       "3            beautiful song anyone could use pick tonight  \n",
       "4            exactly ive told reminds house hansel gretel  \n",
       "...                                                   ...  \n",
       "999995                follow saga suppose mug hard guilty  \n",
       "999996                                         woke angry  \n",
       "999997                           ranch way italian either  \n",
       "999998                                    edit f multiply  \n",
       "999999                           lovely night emily today  \n",
       "\n",
       "[1000000 rows x 7 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_neg = df[df['target']== 0].sample(500000)\n",
    "df_pos = df[df['target']== 4].sample(500000)\n",
    "df_pos['target'] = 1\n",
    "liste_concat = [df_neg, df_pos]\n",
    "df_sample = pd.concat([df_neg, df_pos], ignore_index=True)\n",
    "df_sample = df_sample.sample(frac=1).reset_index(drop=True)\n",
    "df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = '../Data/sample_dataset.csv'\n",
    "df_sample.to_csv(sample_df, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Projet7Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
